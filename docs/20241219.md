### yarn
>yarn资源池里面的计划策略里的DRF是根据 CPU 和内存公平调度资源
> cdh app-yarn部署![app-yarn部署](C:/Users/LEGION/Desktop/20241219/25fbb2b9-7472-406e-a669-6a0882f5d297.png)
### 代码
```
public class Yx_q {
public static void main(String[] args) {
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
//env.setParallelism(1);
//        // 设置 3s 的 checkpoint 间隔
//        env.enableCheckpointing(3000);
MySqlSource<String> mySqlSource = MySqlSource.<String>builder()
.hostname("192.168.10.130")
.port(3306)
.databaseList("xiaoyang_qin") // 设置捕获的数据库， 如果需要同步整个数据库，请将 tableList 设置为 ".*".
.tableList("xiaoyang_qin.user") // 设置捕获的表
.username("root")
.password("root")
//                .startupOptions(StartupOptions.latest())
.startupOptions(StartupOptions.earliest())
//                .startupOptions(StartupOptions.initial())
.deserializer(new JsonDebeziumDeserializationSchema()) // 将 SourceRecord 转换为 JSON 字符串
.build();
DataStreamSource<String> ds1 = env.fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), "MySQL Source");
ds1.print();
// 4. 配置 Kafka 连接信息以及生产者相关参数
String kafkaBootstrapServers = "192.168.10.128:9092";  // Kafka 集群的连接地址，根据实际情况修改
String kafkaTopic = "yx_q";  // 要发送到的 Kafka 主题名称，可自行定义

        Properties kafkaProps = new Properties();
        kafkaProps.setProperty("bootstrap.servers", kafkaBootstrapServers);

        // 创建 FlinkKafkaProducer，用于将数据发送到 Kafka
        FlinkKafkaProducer<String> kafkaProducer = new FlinkKafkaProducer<>(
                kafkaTopic,
                new SimpleStringSchema(),
                kafkaProps
        );

        // 5. 将数据流中的数据发送到 Kafka
        ds1.addSink(kafkaProducer);

        try {
            env.execute();
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

}
```

sync 数据同步 把内存里的数据加载到服务器的磁盘里面去

### 逐字稿
> 面试官你好我叫秦肖杨,目前从事数据开发已经有5年以上的开发经验了
> 对于线上产品阿里的dataworks,dataplin 线下集群CDH CDP都能够完成
> 能独立的搭建部署压测调优上线,开发语言主要使用java和python
> java主要用于写一些flinksql,做一些flink项目,也可以实现从0-1的开发,
> pyhon主要用于爬虫，数据的清洗,脱敏,还有使用Python
> 用于数据预处理、脚本编写与自动化任务执行，如数据采集脚本、
> 数据验证脚本等等以上就是我的一个简单的自我介绍 谢谢面试官

### 项目
>我们这个月主要写的是flink实时的电商项目,我们分了三层架构分别为dim层,
> dwd层和dws层
#### dim层 
>主流是全部表的数据 flinkcdc读取配置表的信息（配置表字段包括来源表,以及来源表的字段),
> 然后通过广播流和主流回合,过滤出维度表的数据存入hbase
#### dwd 层
>dwd层主要操作各种 域 例如 浏览域,交易域 主要是服务dws层, dwd层也是一样的
> 读取hbase的维度表的数据和主流的数据关联过滤出需要的数据打宽表存入不同的kafka的topic中
#### dws层 
>dws层就是求各种指标
> 前面我们已经把各种域的信息存入kafka中 维度信息存入hbase中
> 然后根据指标需求去读取dwd的宽表 如果需要用于维度字段就需要关联hbase中维度表
> 现在的电商项目是通过异步io实现的
#### 最后就是把求到的指标存入到doris中 然后通过suger大屏展示效果